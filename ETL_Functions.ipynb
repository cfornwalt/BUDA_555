{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47391e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jul  7 11:01:20 2022\n",
    "\n",
    "@author: d-carter.fornwalt\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import json\n",
    "import os\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "        \n",
    "def gen_wordcloud(sub_compile, f):\n",
    "    text = \" \".join(log for log in sub_compile.log.astype(str))\n",
    "    # generate word cloud\n",
    "    wordcloud = WordCloud(stopwords = STOPWORDS, collocations=True, max_font_size=50, max_words=20, background_color=\"white\").generate(str(text))\n",
    "    #use words_ to print relative word frequencies\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig((('images/'+f+'/'+f+'_wordcloud.png')), format=\"png\")\n",
    "    plt.show()\n",
    "\n",
    "# Create breakdown of log counts by container and error type\n",
    "def gen_log_counts(fin, log_types):\n",
    "    \n",
    "    nums = pd.DataFrame(columns=['container', 'container_total','log_class','log_class_freq'])\n",
    "    for i in fin['container_name'].unique():\n",
    "        sub = fin.loc[fin['container_name'] == i]\n",
    "        c = list(fin['container_name']).count(i)\n",
    "        for j in log_types:\n",
    "            #c = list(new2me['container_name']).count(i)\n",
    "            b =  list(sub['log_class']).count(j)\n",
    "            d = {'container':[i],'container_total':[c],'log_class':[j],'log_class_freq':[b]}\n",
    "            t = pd.DataFrame(d, columns=['container', 'container_total','log_class','log_class_freq'])\n",
    "            nums = nums.append(t)\n",
    "    return(nums)\n",
    "\n",
    "\n",
    "def plot_log_counts(nums, f):\n",
    "    # Set Up Stacked Bars\n",
    "    \n",
    "    Container = nums['container'].unique()\n",
    "    Error = nums['log_class_freq'].loc[nums['log_class'] == 'Error']\n",
    "    Failure = nums['log_class_freq'].loc[nums['log_class'] == 'Failure']\n",
    "    HealthCheck = nums['log_class_freq'].loc[nums['log_class'] == 'Health Check']\n",
    "    InfrastructureLog = nums['log_class_freq'].loc[nums['log_class'] == 'Infrastructure']\n",
    "    OtherTraffic = nums['log_class_freq'].loc[nums['log_class'] == 'Other']\n",
    "    \n",
    "    # Define width of stacked chart and dimensions \n",
    "    w = 0.6\n",
    "    figure(figsize=(9,6))\n",
    "    # Plot stacked bar chart\n",
    "    \n",
    "    plt.bar(Container, Error, w)\n",
    "    plt.bar(Container, OtherTraffic, w, bottom=Error)\n",
    "    plt.bar(Container, HealthCheck, w, bottom=OtherTraffic+Error)\n",
    "    plt.bar(Container, InfrastructureLog, w, bottom=HealthCheck+OtherTraffic+Error)\n",
    "    plt.bar(Container, Failure, w, bottom=InfrastructureLog+HealthCheck+OtherTraffic+Error)\n",
    "    \n",
    "    # Display\n",
    "    plt.xlabel(\"Container Names\")\n",
    "    title = 'Log Classification Spread for Logs Dated: '+f[:-11]\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Count of Unique Logs\")\n",
    "    plt.legend([\"Error\",\"Other\",\"Health Check\",\"Infrastructure\",\"Failure\"])\n",
    "    plt.ylim(0, nums['container_total'].max()+(nums['container_total'].max()*0.1))\n",
    "    fig1 = plt.gcf()\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    fig1.savefig('images/'+f+'/'+f+'_barchart.png')\n",
    "    \n",
    "    return('PNG of Chart Saved')\n",
    "\n",
    "\n",
    "def label_log(fin, f):\n",
    "    path = 'analyzed/'+f[:-5]\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "    clone = pd.DataFrame()\n",
    "    for i in fin.iterrows():\n",
    "        i = i[1].to_frame().transpose()\n",
    "        log = i['log']\n",
    "        if 'error' in str(log).lower():\n",
    "            i['log_class'] = 'Error'\n",
    "        elif 'fail' in str(log).lower():\n",
    "            i['log_class'] = 'Failure'\n",
    "        elif 'health' in str(log).lower():\n",
    "            i['log_class'] = 'Health Check'\n",
    "        elif 'datacenter' in str(log).lower():\n",
    "            i['log_class'] = 'Infrastructure'\n",
    "        else:\n",
    "            i['log_class'] = 'Other'\n",
    "        clone = pd.concat([clone, i])\n",
    "    f = path+'/'+f+'_analyzed.csv'\n",
    "    clone.to_csv(f)\n",
    "    return(clone)\n",
    "\n",
    "def convertJsontoDataframe(df):\n",
    "    '''\n",
    "    param: df - dataframe of detail columns and one large complex json\n",
    "    return: dataframe with all nested json objects translated to columns\n",
    "    \n",
    "    This is a super ugly function to process the input dataframe, which consists\n",
    "    of a few fields and one large dictionary. This was written using the available\n",
    "    data, and some hardcoded fields may need changed in the future\n",
    "    '''\n",
    "    # skeletons\n",
    "    try:\n",
    "        print(len(df))\n",
    "        print(df.columns)\n",
    "        docks = []\n",
    "        dock_sub = []\n",
    "        kubes = []\n",
    "        kube_sub = []\n",
    "        remainder = []\n",
    "        if '_source' in df.columns:\n",
    "            source = zip(list(df['_id']), list(df['_source']))    \n",
    "        else:\n",
    "            source = zip(list(df['hits']['_id']), list(df['hits']['_source']))\n",
    "            \n",
    "        for h, i in source:\n",
    "            # set up copy\n",
    "            c = i\n",
    "            c['_id'] = h\n",
    "            # do docker\n",
    "            if 'docker' in i.keys():\n",
    "                docker = i['docker']\n",
    "                docker['_id'] = h\n",
    "                # look for additional json objects\n",
    "                for k, v in docker.items():\n",
    "                    if isinstance(v, dict):\n",
    "                        a = docker[k]\n",
    "                        a['_id'] = h\n",
    "                        dock_sub.append(a)\n",
    "                \n",
    "                docks.append(docker)\n",
    "                c.pop('docker')\n",
    "            # time for kubernetes\n",
    "            if 'kubernetes' in i.keys():\n",
    "                \n",
    "                kubernetes = i['kubernetes']\n",
    "                kubernetes['_id'] = h\n",
    "                # look for additional json objects\n",
    "                for k, v in kubernetes.items():\n",
    "                    if isinstance(v, dict):\n",
    "                        a = kubernetes[k]\n",
    "                        a['_id'] = h\n",
    "                        kube_sub.append(a)\n",
    "                        \n",
    "                kubes.append(kubernetes)\n",
    "                c.pop('kubernetes')\n",
    "            #append non dicts\n",
    "            remainder.append(c)\n",
    "        # run all lists through compiler\n",
    "        final = compileDataframes(df, remainder, kubes, kube_sub, docks, dock_sub)\n",
    "    except:\n",
    "        print('gave up')\n",
    "    return(final)\n",
    "    \n",
    "    \n",
    "def compileDataframes(df, remainder, kubes, kube_sub, docks, dock_sub):\n",
    "    '''\n",
    "    param: \n",
    "        df - original dataframe object including complex json column\n",
    "        remainder - non-json objects from complex json as a list\n",
    "        kubes - core kubernetes json object as list\n",
    "        kube_Sub - nested objects from kubernetes object, processed separately\n",
    "        docks - core docker json object as list\n",
    "        dock_sub - nested objects from docker object, processed separately\n",
    "    return:\n",
    "        frankenstein - final dataframe with entire complex json organized as columns\n",
    "        \n",
    "    This is a supplementary function to convertJsontoDataframe, meant to combine\n",
    "    all of the resulting dictionary to list translations. This converts lists\n",
    "    consiting of dictionaries to dataframes, merged on the unique log identifier\n",
    "    '''\n",
    "    \n",
    "    remainder = pd.DataFrame(remainder)\n",
    "    kube_sub = pd.DataFrame(kube_sub)\n",
    "    if len(kube_sub) != 0:\n",
    "        kube_sub = kube_sub.groupby('_id').apply(lambda x : x.ffill()).drop_duplicates('_id', keep='last')\n",
    "    kubes = pd.DataFrame(kubes)\n",
    "    docks = pd.DataFrame(docks)\n",
    "    dock_sub = pd.DataFrame(dock_sub)\n",
    "    frankenstein = df.merge(remainder, on='_id')\n",
    "    if len(kubes) != 0:\n",
    "        frankenstein = frankenstein.merge(kubes, on='_id')\n",
    "    if len(docks) != 0:\n",
    "        frankenstein = frankenstein.merge(docks, on='_id')\n",
    "\n",
    "    if len(dock_sub) != 0:\n",
    "        frankenstein = frankenstein.merge(dock_sub, on='_id')\n",
    "    if len(kube_sub) != 0:\n",
    "        frankenstein = frankenstein.merge(kube_sub, on='_id')\n",
    "    cols = ['_source', 'labels','namespace_labels']\n",
    "    for column in cols:\n",
    "        if column in frankenstein.columns:\n",
    "            frankenstein = frankenstein.drop(columns=[column])\n",
    "    \n",
    "    return(frankenstein)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
